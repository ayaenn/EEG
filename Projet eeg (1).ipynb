{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eccc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load Metadata from Participants.tsv\n",
    "participants_file = '/Users/pro/Desktop/BigData/ProjetEEG/ds004504-1.0.7/participants.tsv'\n",
    "participants_df = pd.read_csv(participants_file, sep='\\t')\n",
    "\n",
    "# Keep only relevant columns: participant_id and group \n",
    "participants_df = participants_df[['participant_id', 'Group']]\n",
    "\n",
    "# Create a dictionary to map participant_id to group\n",
    "group_mapping = participants_df.set_index('participant_id')['Group'].to_dict()\n",
    "\n",
    "# Step 2: Load and Segment EEG Signals with Overlap\n",
    "# Directory containing the EEG data\n",
    "data_dir = '/Users/pro/Desktop/BigData/ProjetEEG/ds004504-1.0.7/derivatives'\n",
    "\n",
    "# Parameters for segmentation\n",
    "segment_length_sec = 5  # length of each segment in seconds\n",
    "sampling_rate = 500  # Hz\n",
    "segment_length = segment_length_sec * sampling_rate  # Number of data points per segment\n",
    "\n",
    "# Parameters for overlap\n",
    "overlap_ratio = 0.5  # 50% overlap\n",
    "overlap_step = int(segment_length * (1 - overlap_ratio))  # Step size considering overlap\n",
    "\n",
    "data_segments = []\n",
    "group_labels = []\n",
    "\n",
    "# Iterate over each subject folder to load EEG data\n",
    "directories = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d)) and d != '.DS_Store']\n",
    "print(\"Directories in data_dir (Total):\", len(directories))\n",
    "print(\"Directories in data_dir:\", directories)\n",
    "\n",
    "for participant_id in directories:\n",
    "    participant_folder = os.path.join(data_dir, participant_id)\n",
    "    \n",
    "    if participant_id in group_mapping:\n",
    "        # Load the EEG data\n",
    "        eeg_file = os.path.join(participant_folder, 'eeg', f'{participant_id}_task-eyesclosed_eeg.set')\n",
    "        print(\"Looking for EEG file:\", eeg_file)\n",
    "        \n",
    "        if os.path.exists(eeg_file):\n",
    "            print(f\"Loading EEG data for {participant_id}...\")\n",
    "            raw = mne.io.read_raw_eeglab(eeg_file, preload=True)\n",
    "            \n",
    "            # Get the EEG data as a numpy array\n",
    "            data = raw.get_data()  # Shape: (n_channels, n_samples)\n",
    "            n_channels, n_samples = data.shape\n",
    "            \n",
    "            # Segment the data with overlap\n",
    "            start = 0\n",
    "            while start + segment_length <= n_samples:\n",
    "                end = start + segment_length\n",
    "                segment = data[:, start:end]\n",
    "                \n",
    "                # Append the segment and corresponding group label\n",
    "                data_segments.append(segment)\n",
    "                group_labels.append(group_mapping[participant_id])\n",
    "                \n",
    "                # Move to the next segment considering overlap\n",
    "                start += overlap_step\n",
    "\n",
    "        else:\n",
    "            print(f\"EEG file not found for participant: {participant_id}\")\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "data_segments = np.array(data_segments)  # Shape: (n_segments, n_channels, segment_length)\n",
    "group_labels = np.array(group_labels)\n",
    "\n",
    "# Print the shapes of the segmented data and labels\n",
    "print(\"Data Segments Shape (with overlap):\", data_segments.shape)\n",
    "print(\"Group Labels Shape:\", group_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1933cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 3: Standardize Each Segment\n",
    "scaler = StandardScaler()\n",
    "standardized_segments = []\n",
    "\n",
    "for segment in data_segments:\n",
    "    # Standardize each channel independently\n",
    "    standardized_segment = scaler.fit_transform(segment)\n",
    "    standardized_segments.append(standardized_segment)\n",
    "\n",
    "# Convert the standardized segments list to a numpy array\n",
    "standardized_segments = np.array(standardized_segments)\n",
    "\n",
    "print(\"Standardized Segments Shape:\", standardized_segments.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05bebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Step 4: Define and Train the Autoencoder\n",
    "input_shape = standardized_segments.shape[1:]  # Shape: (n_channels, segment_length)\n",
    "latent_dim = 32  \n",
    "\n",
    "# Define the encoder part of the autoencoder\n",
    "input_layer = Input(shape=input_shape)\n",
    "flattened = tf.keras.layers.Flatten()(input_layer)\n",
    "encoded = Dense(128, activation='relu')(flattened)\n",
    "encoded = Dense(latent_dim, activation='relu')(encoded)\n",
    "\n",
    "# Define the decoder part of the autoencoder\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(np.prod(input_shape), activation='linear')(decoded)\n",
    "decoded = tf.keras.layers.Reshape(input_shape)(decoded)\n",
    "\n",
    "# Build the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(\n",
    "    standardized_segments, \n",
    "    standardized_segments,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Define the encoder model to extract features\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# Extract features for all segments\n",
    "encoded_segments = encoder.predict(standardized_segments)\n",
    "\n",
    "# Print the shape of the encoded segments\n",
    "print(\"Encoded Segments Shape:\", encoded_segments.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df19945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "# Définition des bandes de fréquence (incluant Gamma)\n",
    "frequency_bands = {\n",
    "    'delta': (0.5, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 13),\n",
    "    'beta': (13, 25),\n",
    "    'gamma': (25, 45)\n",
    "}\n",
    "\n",
    "# Taux d'échantillonnage des données EEG (en Hz)\n",
    "sampling_rate = 128\n",
    "\n",
    "def extract_band_power(segments, frequency_bands, sampling_rate):\n",
    "    \"\"\"\n",
    "    Extrait la puissance des bandes de fréquence pour chaque segment EEG encodé.\n",
    "    \n",
    "    Args:\n",
    "        segments (np.array): Segments encodés (N_segments, N_features)\n",
    "        frequency_bands (dict): Dictionnaire des bandes de fréquence à extraire\n",
    "        sampling_rate (int): Taux d'échantillonnage des données EEG\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Puissances des bandes de fréquence pour chaque segment\n",
    "    \"\"\"\n",
    "    band_powers = []\n",
    "    for segment in segments:\n",
    "        # Calcul du spectre de puissance (PSD)\n",
    "        freqs, psd = signal.welch(segment, fs=sampling_rate, nperseg=len(segment))\n",
    "        \n",
    "        # Calcul de la puissance pour chaque bande de fréquence définie\n",
    "        band_power = []\n",
    "        for band, (low_freq, high_freq) in frequency_bands.items():\n",
    "            idx_band = np.logical_and(freqs >= low_freq, freqs <= high_freq)\n",
    "            power = integrate.simps(psd[idx_band], freqs[idx_band])\n",
    "            band_power.append(power)\n",
    "        \n",
    "        band_powers.append(band_power)\n",
    "    \n",
    "    return np.array(band_powers)\n",
    "\n",
    "# Calcul des bandes de puissance sur les segments encodés\n",
    "band_powers = extract_band_power(encoded_segments, frequency_bands, sampling_rate)\n",
    "\n",
    "print(\"Band Powers Shape:\", band_powers.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be898541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "# Calcul de l'entropie spectrale à partir des bandes de puissance\n",
    "def calculate_spectral_entropy(band_powers):\n",
    "    \"\"\"\n",
    "    Calculer l'entropie spectrale pour chaque segment encodé.\n",
    "\n",
    "    Arguments:\n",
    "    band_powers -- numpy array de forme (n_samples, n_bands)\n",
    "\n",
    "    Retourne:\n",
    "    spectral_entropy -- numpy array de forme (n_samples,)\n",
    "    \"\"\"\n",
    "    # Normaliser les bandes de puissance pour obtenir des probabilités\n",
    "    normalized_powers = band_powers / np.sum(band_powers, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculer l'entropie pour chaque segment\n",
    "    spectral_entropy = entropy(normalized_powers, axis=1)\n",
    "    \n",
    "    return spectral_entropy\n",
    "\n",
    "# Calculer l'entropie spectrale sur les bandes de puissance extraites\n",
    "spectral_entropy = calculate_spectral_entropy(band_powers)\n",
    "\n",
    "print(\"Spectral Entropy Shape:\", spectral_entropy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbf408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinaison des caractéristiques : bandes de puissance + entropie spectrale\n",
    "combined_features = np.hstack((band_powers, spectral_entropy.reshape(-1, 1)))  # Forme : (27032, 6)\n",
    "print(\"Combined Features Shape:\", combined_features.shape)\n",
    "\n",
    "# Préparation des séquences pour le LSTM\n",
    "sequence_length = 10  # Nombre de segments consécutifs par séquence\n",
    "num_features = combined_features.shape[1]\n",
    "\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(combined_features) - sequence_length):\n",
    "    sequence = combined_features[i:i + sequence_length]\n",
    "    label = group_labels[i + sequence_length - 1]  \n",
    "    sequences.append(sequence)\n",
    "    labels.append(label)\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(\"Sequences Shape:\", sequences.shape)  \n",
    "print(\"Labels Shape:\", labels.shape)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Encode labels into integers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Step 2: One-hot encode the labels for categorical classification\n",
    "categorical_labels = to_categorical(encoded_labels)\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sequences, \n",
    "    categorical_labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=encoded_labels  \n",
    ")\n",
    "\n",
    "# Step 4: Compute class weights to handle class imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(encoded_labels),\n",
    "    y=encoded_labels\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Step 5: Define the optimized LSTM model\n",
    "model_optimized = Sequential([\n",
    "    Bidirectional(LSTM(128, return_sequences=True, activation='tanh'), input_shape=(sequence_length, num_features)),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    LSTM(64, return_sequences=False, activation='tanh'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(categorical_labels.shape[1], activation='softmax')  # Output layer with softmax for classification\n",
    "])\n",
    "\n",
    "model_optimized.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Add EarlyStopping and ReduceLROnPlateau callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "# Step 7: Train the model with class weights\n",
    "history = model_optimized.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,  # Train for more epochs with EarlyStopping\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights_dict,  # Handle class imbalance\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "# Step 8: Evaluate the optimized model on the test set\n",
    "test_loss, test_accuracy = model_optimized.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss (Optimized): {test_loss}\")\n",
    "print(f\"Test Accuracy (Optimized): {test_accuracy}\")\n",
    "\n",
    "# Step 9: Save the trained optimized model\n",
    "# Step 9: Save the trained optimized model\n",
    "model_optimized.save(\"/Users/pro/Desktop/BigData/ProjetEEG/model/eeg_model_for5s.h5\")\n",
    "print(\"Optimized Model saved as '/Users/pro/Desktop/BigData/ProjetEEG/model/eeg_model_for5s.h5'\")\n",
    "\n",
    "\n",
    "# Step 10: Use the optimized model for predictions\n",
    "predictions = model_optimized.predict(X_test)\n",
    "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "true_labels = label_encoder.inverse_transform(np.argmax(y_test, axis=1))\n",
    "\n",
    "# Print a few predictions and true labels for verification\n",
    "for i in range(10):\n",
    "    print(f\"Predicted (Optimized): {predicted_labels[i]}, True: {true_labels[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
